ipynb-py-convert XX.py XX.ipynb

1.token_sent.py
from nltk.tokenize import sent_tokenize
sentence = sent_tokenize("Hello NLP learner. This is an NLP python script")
print (sentence)

2. token_word.py
from nltk.tokenize import word_tokenize
words = word_tokenize("We are learning Artificial Intelligence")
print (words

3. pos_tag.py
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
words = word_tokenize("We are learning Artificial Intelligence")
print (words)
pos_words = pos_tag(words)
print (pos_words)

4. chunk.py
from nltk.chunk import ne_chunk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
chunk_sentence = ne_chunk(pos_tag(word_tokenize('Steve Jobs was an ex CEO of Apple Corporation at California.'))) 
print (chunk_sentence)
chunk_sentence.draw()

5. stem.py

from nltk.stem import PorterStemmer
#from nltk.tokenize import sent_tokenize, word_tokenize
ps = PorterStemmer()
list_words = ["python","pythoner","pythoning","pythoned","pythonly","wolves", "leaves","horses","dogs","fairly"]
for word in list_words:
    print(ps.stem(word))

6. from nltk import stemwn_lemat = stem.WordNetLemmatizer()
print (wn_lemat.lemmatize('dogs'))
print (wn_lemat.lemmatize('leaves'))
print (wn_lemat.lemmatize('wolves'))
print (wn_lemat.lemmatize('babies'))
print (wn_lemat.lemmatize('geese'))

7a.  sentiment,py
senti_dict = {​​​​​​​}​​​​​​​
for each_line in open('./trng_data/mysenti.txt'):
    word,score = each_line.split('\t')
    senti_dict[word] = int(score)
words = 'This is not a good training and i appreciate it. But lab machines are worst'.lower().split()

print(words)
print(sum( senti_dict.get(word, 0) for word in words ))

7b. mysenti.txt
like    1
good    1
appreciate    2
best    3
bad    -1
worst    -2
terrible    -3
