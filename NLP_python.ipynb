{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize as sk\n",
    "df=pd.read_excel(\"data_nlp1.xlsx\")\n",
    "k=df['Comment']\n",
    "#print(k[0])\n",
    "count= len(k)\n",
    "st=[]\n",
    "sent=[]\n",
    "#print(ds)\n",
    "for i in range(count):\n",
    "    st.append(k[i])\n",
    "    sent.append(sk(st[i]))\n",
    "ds = pd.DataFrame(sent)\n",
    "writer=pd.ExcelWriter(\"data_out.xlsx\")                #data_out is the result for sentence tokenize\n",
    "ds.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize as wk\n",
    "df=pd.read_excel(\"data_nlp1.xlsx\")\n",
    "k=df['Comment']\n",
    "#print(k[0])\n",
    "count= len(k)\n",
    "st=[]\n",
    "sent=[]\n",
    "#print(ds)\n",
    "for i in range(count):\n",
    "    st.append(k[i])\n",
    "    sent.append(wk(st[i]))\n",
    "ds = pd.DataFrame(sent)\n",
    "writer=pd.ExcelWriter(\"data_out2.xlsx\")   #data_out2 is the result for words tokenize\n",
    "ds.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', ',', 'how', 'are', 'you', '?', 'Weather', 'is', 'awesome', '.', 'Its', 'raining', 'here', 'now', '.']\n",
      "[('Hello', 'NNP'), ('there', 'RB'), (',', ','), ('how', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('?', '.'), ('Weather', \"''\"), ('is', 'VBZ'), ('awesome', 'JJ'), ('.', '.'), ('Its', 'PRP$'), ('raining', 'VBG'), ('here', 'RB'), ('now', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "words = word_tokenize(\"Hello there, how are you? Weather is awesome. Its raining here now.\")\n",
    "print (words)\n",
    "pos_words = pos_tag(words)\n",
    "print (pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS-Tagging and tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "f = open(\"NLP3_and_4.txt\", \"r\")\n",
    "arr=f.read().split('\\n')\n",
    "print(len(arr))\n",
    "for i in range (len(arr)):\n",
    "    chunk_sentence = ne_chunk(pos_tag(word_tokenize(arr[i])))\n",
    "    #print(chunk_sentence)\n",
    "    chunk_sentence.draw()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmantizing and steming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "there\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      "?\n",
      "weather\n",
      "is\n",
      "awesom\n",
      ".\n",
      "it\n",
      "rain\n",
      "here\n",
      "now\n",
      ".\n",
      "------------\n",
      "hello\n",
      "mr.\n",
      "raja\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      "?\n",
      "weather\n",
      "is\n",
      "awesom\n",
      ".\n",
      "it\n",
      "rain\n",
      "here\n",
      "now\n",
      ".\n",
      "------------\n",
      "hello\n",
      "mr.\n",
      "raja\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      ".\n",
      "weather\n",
      "is\n",
      "bad\n",
      ".\n",
      "it\n",
      "heavili\n",
      "rain\n",
      "here\n",
      "now\n",
      ".\n",
      "------------\n",
      "nlp\n",
      "is\n",
      "great\n",
      "techniqu\n",
      ".\n",
      "It\n",
      "is\n",
      "nice\n",
      "to\n",
      "learn\n",
      "thi\n",
      "techniqu\n",
      ".\n",
      "------------\n",
      "AI\n",
      "is\n",
      "make\n",
      "differ\n",
      "in\n",
      "thi\n",
      "world\n",
      "now\n",
      ".\n",
      "It\n",
      "would\n",
      "be\n",
      "help\n",
      "for\n",
      "better\n",
      "of\n",
      "human\n",
      "life\n",
      ".\n",
      "We\n",
      "need\n",
      "to\n",
      "make\n",
      "advantag\n",
      "of\n",
      "that\n",
      ".\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "ps = PorterStemmer()\n",
    "f = open(\"NLP3_and_4.txt\", \"r\")\n",
    "arr=f.read().split('\\n')\n",
    "for i in range(len(arr)):\n",
    "    kk=word_tokenize(arr[i])\n",
    "    #print(kk)\n",
    "    #print(\"\\n\")\n",
    "    count = len(word_tokenize(arr[i]))\n",
    "    for j in range(count):\n",
    "        print (ps.stem(kk[j]))\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hello\n",
      "there\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      "?\n",
      "Weather\n",
      "is\n",
      "awesome\n",
      ".\n",
      "Its\n",
      "raining\n",
      "here\n",
      "now\n",
      ".\n",
      "------------\n",
      "\n",
      "\n",
      "Hello\n",
      "Mr.\n",
      "Raja\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      "?\n",
      "Weather\n",
      "is\n",
      "awesome\n",
      ".\n",
      "Its\n",
      "raining\n",
      "here\n",
      "now\n",
      ".\n",
      "------------\n",
      "\n",
      "\n",
      "Hello\n",
      "Mr.\n",
      "Raja\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      ".\n",
      "Weather\n",
      "is\n",
      "bad\n",
      ".\n",
      "Its\n",
      "heavily\n",
      "raining\n",
      "here\n",
      "now\n",
      ".\n",
      "------------\n",
      "\n",
      "\n",
      "NLP\n",
      "is\n",
      "great\n",
      "technique\n",
      ".\n",
      "It\n",
      "is\n",
      "nice\n",
      "to\n",
      "learn\n",
      "this\n",
      "technique\n",
      ".\n",
      "------------\n",
      "\n",
      "\n",
      "AI\n",
      "is\n",
      "making\n",
      "difference\n",
      "in\n",
      "this\n",
      "world\n",
      "now\n",
      ".\n",
      "It\n",
      "would\n",
      "be\n",
      "helpful\n",
      "for\n",
      "betterment\n",
      "of\n",
      "human\n",
      "life\n",
      ".\n",
      "We\n",
      "need\n",
      "to\n",
      "make\n",
      "advantage\n",
      "of\n",
      "that\n",
      ".\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "import string\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "f = open(\"NLP3_and_4.txt\", \"r\")\n",
    "arr=f.read().split('\\n')\n",
    "for i in range(len(arr)):\n",
    "    kk=word_tokenize(arr[i])\n",
    "    #print(kk)\n",
    "    print(\"\\n\")\n",
    "    count = len(word_tokenize(arr[i]))\n",
    "    for j in range(count):\n",
    "        print (lemmatizer.lemmatize(kk[j]))\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hate': -1, 'hatred': -2, 'annoyed': -1, 'annoy': -1, 'annoyingly': -2, 'nasty': -3, 'nice': 1, 'excellent': 3, 'good': 2, 'wonderful': 3, 'best': 3, 'better': 2, 'awesome': 2, 'beautiful': 1, 'beauty': 1, 'beautifully': 1, 'supreme': 2}\n",
      "['Rose', 'is', 'beautiful', '.']\n",
      "1\n",
      "['Place', 'is', 'nasty', 'to', 'stay', '.']\n",
      "-3\n",
      "['This', 'is', 'the', 'beauty', 'of', 'this', 'technique', '.']\n",
      "1\n",
      "['Concept', 'is', 'explained', 'beautifully', 'in', 'this', 'book', '.']\n",
      "1\n",
      "['He', 'annoyed', 'me', '.']\n",
      "-1\n",
      "['Its', 'the', 'supreme', 'place', 'to', 'stay', '.']\n",
      "2\n",
      "['I', 'hate', 'this', 'place', '.']\n",
      "-1\n",
      "['Dont', 'annoy', 'the', 'customer', '.']\n",
      "-1\n",
      "['He', 'has', 'given', 'nasty', 'comments', 'about', 'his', 'stay', '.']\n",
      "-3\n",
      "['Dessert', 'is', 'awesome', '.']\n",
      "2\n",
      "['Your', 'gift', 'is', 'wonderful', '.']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "senti_dict = {}\n",
    "for each_line in open('NLP5_weight.txt'):\n",
    "    word,score = each_line.split(\"    \")\n",
    "    senti_dict[word] = int(score)\n",
    "print(senti_dict)\n",
    "f = open(\"NLP5_texts.txt\", \"r\")\n",
    "arr=f.read().split('\\n')\n",
    "arr\n",
    "for i in range(len(arr)):\n",
    "    words=word_tokenize(arr[i])\n",
    "    print(words)\n",
    "    print(sum( senti_dict.get(word, 0) for word in words ))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
